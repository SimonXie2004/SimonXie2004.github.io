---
title: Fun with Diffusion Models
mathjax: true
date: 2024-11-19 23:00:16
tags:
- Computer Vision
- Deep Learning
- Generative Models
category: UCB-CV-Project
header_image:
abstract: UC Berkeley CV Project 5a -  Fun with Diffusion Models & 5b - Implement DDPM Yourself
---

# Project 5a & 5b

> UC Berkeley CV Project 5a: Fun with Diffusion Models
>
> UC Berkeley CV Project 5b: Implement DDPM Yourself

## Part A: Fun with Diffusion Models

> Remark: For all subsequent sections in part A, we use `20040805` as our seed. 
>
> We will call `seed()` every time before we generate pictures to make **all** the results reproducible.

### Setup

First, we setup the [DeepFloyd IF](https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if) diffusion model.

Here are some example images generated by DeepFloydIF, using 3 different text prompts and different `num_inference_steps`. It can be read from the results that with higher num_inference_steps the image is more complete.

<table>
  <tr>
    <th>Inference step \ <br> Text prompt</th>
    <th>An oil painting of a<br>snowy mountain village</th>
    <th>A man wearing a hat</th>
    <th>A rocket ship</th>
  </tr>
  <tr>
    <td>Step=2</td>
    <td><img src="/images/DDPM/village1.png" alt="Example Image 1-1" width="100"></td>
    <td><img src="/images/DDPM/man_hat1.png" alt="Example Image 2-1" width="100"></td>
    <td><img src="/images/DDPM/rocket1.png" alt="Example Image 3-1" width="100"></td>
  </tr>
  <tr>
    <td>Step=4</td>
    <td><img src="/images/DDPM/village2.png" alt="Example Image 1-2" width="100"></td>
    <td><img src="/images/DDPM/man_hat2.png" alt="Example Image 2-2" width="100"></td>
    <td><img src="/images/DDPM/rocket2.png" alt="Example Image 3-2" width="100"></td>
  </tr>
  <tr>
    <td>Step=6</td>
    <td><img src="/images/DDPM/village3.png" alt="Example Image 1-3" width="100"></td>
    <td><img src="/images/DDPM/man_hat3.png" alt="Example Image 2-3" width="100"></td>
    <td><img src="/images/DDPM/rocket3.png" alt="Example Image 3-3" width="100"></td>
  </tr>
  <tr>
    <td>Step=8</td>
    <td><img src="/images/DDPM/village4.png" alt="Example Image 1-4" width="100"></td>
    <td><img src="/images/DDPM/man_hat4.png" alt="Example Image 2-4" width="100"></td>
    <td><img src="/images/DDPM/rocket4.png" alt="Example Image 3-4" width="100"></td>
  </tr>
  <tr>
    <td>Step=12</td>
    <td><img src="/images/DDPM/village5.png" alt="Example Image 1-5" width="100"></td>
    <td><img src="/images/DDPM/man_hat5.png" alt="Example Image 2-5" width="100"></td>
    <td><img src="/images/DDPM/rocket5.png" alt="Example Image 3-5" width="100"></td>
  </tr>
  <tr>
    <td>Step=16</td>
    <td><img src="/images/DDPM/village6.png" alt="Example Image 1-6" width="100"></td>
    <td><img src="/images/DDPM/man_hat6.png" alt="Example Image 2-6" width="100"></td>
    <td><img src="/images/DDPM/rocket6.png" alt="Example Image 3-6" width="100"></td>
  </tr>
</table>

### Forward Process

A key part of diffusion is the forward process, which takes a clean image and adds noise to it. In this part, we will write a function to implement this. The forward process is defined by:
$$
q(x_t|x_0) = N(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)\mathbf{I})
$$
Which is equivalent to computing:
$$
x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon \quad \text{where} \quad \epsilon \sim N(0, 1)
$$
That is, given a clean image $x_0$, we get a noisy image $x_t$ at timestep $t$ by sampling from a Gaussian with mean $\sqrt{\bar{a_t}}x_0$ and variance $\sqrt{1-\bar{a_t}}$.

Here is an example showing the process of adding noise to a single image.

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th></th>
    <th>t=000</th>
    <th>t=125</th>
    <th>t=250</th>
    <th>t=375</th>
    <th>t=500</th>
    <th>t=625</th>
    <th>t=750</th>
    <th>t=875</th>
    <th>t=999</th>
  </tr>
  <tr>
    <td>Noisy <br> Image</td>
    <td><img src="/images/DDPM/campanile_noise1.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise2.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise3.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise4.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise5.png" alt="Image 5" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise6.png" alt="Image 6" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise7.png" alt="Image 7" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise8.png" alt="Image 8" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise9.png" alt="Image 9" width="100"></td>
  </tr>
</table>

### Classical Denoising

Let's try to denoise these images using classical methods. Again, take noisy images, but use **Gaussian blur filtering** to try to remove the noise. Getting good results should be quite difficult, if not impossible.

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th></th>
    <th>t=000</th>
    <th>t=125</th>
    <th>t=250</th>
    <th>t=375</th>
    <th>t=500</th>
    <th>t=625</th>
    <th>t=750</th>
    <th>t=875</th>
    <th>t=999</th>
  </tr>
  <tr>
    <td>Noisy <br> Image</td>
    <td><img src="/images/DDPM/campanile_noise1.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise2.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise3.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise4.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise5.png" alt="Image 5" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise6.png" alt="Image 6" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise7.png" alt="Image 7" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise8.png" alt="Image 8" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise9.png" alt="Image 9" width="100"></td>
  </tr>
  <tr>
    <td>Blurr Image</td>
    <td><img src="/images/DDPM/campanile_noise_gaus1.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_gaus2.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_gaus3.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_gaus4.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_gaus5.png" alt="Image 5" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_gaus6.png" alt="Image 6" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_gaus7.png" alt="Image 7" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_gaus8.png" alt="Image 8" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_gaus9.png" alt="Image 9" width="100"></td>
  </tr>
</table>


### One-Step Denoising

Now, we'll use a pretrained diffusion model to denoise. The actual denoiser is `stage_1.unet`. This is a UNet that has already been trained on a **very, very** large dataset of $(x_0,x_t)$ pairs of images. We can use it to recover Gaussian noise from the image. Then, we can remove this noise to recover (something close to) the original image. The denoising process is defined as:
$$
x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon \right) \quad \text{where} \quad \epsilon \sim N(0, 1)
$$
Note1: This UNet is conditioned on the amount of Gaussian noise by taking timestep t as additional input.

Note2: Because this diffusion model was trained with text conditioning, we also need a text prompt embedding. We use the prompt embedding `"a high quality photo"` generated by **T5 Text Encoder**.

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th></th>
    <th>t=000</th>
    <th>t=125</th>
    <th>t=250</th>
    <th>t=375</th>
    <th>t=500</th>
    <th>t=625</th>
    <th>t=750</th>
    <th>t=875</th>
    <th>t=999</th>
  </tr>
  <tr>
    <td>Noisy<br>Image</td>
    <td><img src="/images/DDPM/campanile_noise1.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise2.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise3.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise4.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise5.png" alt="Image 5" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise6.png" alt="Image 6" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise7.png" alt="Image 7" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise8.png" alt="Image 8" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise9.png" alt="Image 9" width="100"></td>
  </tr>
  <tr>
    <td>One-step<br>Denoised<br>Image</td>
    <td><img src="/images/DDPM/campanile_noise_onestep1.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_onestep2.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_onestep3.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_onestep4.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_onestep5.png" alt="Image 5" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_onestep6.png" alt="Image 6" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_onestep7.png" alt="Image 7" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_onestep8.png" alt="Image 8" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_onestep9.png" alt="Image 9" width="100"></td>
  </tr>
</table>

### Iterative Denoising

In the previous section, we can see that the denoising UNet does a much better job of projecting the image onto the natural image manifold, but it does get worse as you add more noise. This makes sense, as the problem is much harder with more noise.

But diffusion models are designed to denoise iteratively. In this part we will implement this.

In theory, we could start with noise $x_{1000}$ at timestep $T=1000$, denoise for one step to get an estimate of $x_{999}$, and carry on until we get $x_0$. But this would require running the diffusion model 1000 times, which is quite slow (and costs $$$).

It turns out, we can actually speed things up by skipping steps. The rationale for why this is possible is due to a connection with differential equations. Check [this excellent article](https://yang-song.net/blog/2021/score/).

To skip steps we can create a new list of timesteps that we'll call `strided_timesteps`, which does just this. `strided_timesteps` will correspond to the noisiest image (and thus the largest t) and `strided_timesteps[-1]` will correspond to a clean image. One simple way of constructing this list is by introducing a regular stride step (e.g. stride of 30 works well).

On the `i`th denoising step we are at t= `strided_timesteps[i]`, and want to get to t′= `strided_timesteps[i+1]` (from more noisy to less noisy). To actually do this, we have the following formula:

The formula is given as:

$$
x_{t'} = \frac{\sqrt{\bar{\alpha}_{t'} / \beta_t}}{1 - \bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t'})}{1 - \bar{\alpha}_t} x_t + v_\sigma
$$

Where:

- $x_t$ is image at timestep $t$.
- $x_{t'}$ is noisy image at timestep $t'$ where $t' < t$ (less noisy).
- $\bar{\alpha}_t$ is defined by `alphas_cumprod`, as explained above.
- $\alpha_t = \frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}$.
- $\beta_t = 1 - \alpha_t$.
- $x_0$ is our current estimate of the clean image using equation A.2 just like in section 1.3.

The $v_\sigma$ is random noise, which in the case of DeepFloyd is also predicted.

Here is an example result, showing the process of **strided iterative** denoising.

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th></th>
    <th>t=660</th>
    <th>t=510</th>
    <th>t=360</th>
    <th>t=210</th>
    <th>t=60</th>
    <th>t=0</th>
  </tr>
  <tr>
    <td>Noisy <br> Image</td>
    <td><img src="/images/DDPM/campanile_noise_iter1.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_iter2.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_iter3.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_iter4.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_iter5.png" alt="Image 5" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_iter6.png" alt="Image 6" width="100"></td>
  </tr>
</table>

For your comparison, here is a summary of all denoising results, using all aforementioned algorithms.

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th>Original Image</th>
    <th>Gaus Blur Denoised</th>
    <th>One-Step Denoised</th>
    <th>Iterative Denoised</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/campanile_noise_iter1.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_gaus660.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_onestep660.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/campanile_noise_iter6.png" alt="Image 4" width="100"></td>
  </tr>
</table>
### Diffusion Model Sampling

In the previous part, we use the diffusion model to denoise an image. Another thing we can do with the `iterative_denoise` function is to generate images from scratch. We can do this by setting `i_start = 0` and passing in random noise. This effectively denoises pure noise, and we sample 5 results of `"a high quality photo"` from pure noise.

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th>Sample 1</th>
    <th>Sample 2</th>
    <th>Sample 3</th>
    <th>Sample 4</th>
    <th>Sample 5</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/diffusion_sample_1.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/diffusion_sample_2.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/diffusion_sample_3.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/diffusion_sample_4.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/diffusion_sample_5.png" alt="Image 4" width="100"></td>
  </tr>
</table>

### Classifier-Free Guidance (CFG)

You may have noticed that the generated images in the prior section are not very good, and some are completely non-sensical. In order to greatly improve image quality (at the expense of image diversity), we can use a technicque called [Classifier-Free Guidance](https://arxiv.org/abs/2207.12598).

In CFG, we compute both a conditional and an unconditional noise estimate. We denote these $\epsilon_c$ and $\epsilon_u$. Then, we let our new noise estimate be
$$
\epsilon = \epsilon_u + \lambda(\epsilon_c-\epsilon_u)
$$
where $\lambda$ controls the strength of CFG. Notice that for $\lambda=0$, we get an unconditional noise estimate, and for $\lambda=1$ we get the conditional noise estimate. The magic happens when $\lambda>1$. In this case, we get much higher quality images. Why this happens is still up to vigorous debate. For more information on CFG, you can check out [this blog post](https://sander.ai/2022/05/26/guidance.html).

**Disclaimer**: Before, we used `"a high quality photo"` as a "null" condition. Now, we will use the actual `""` null prompt for unconditional guidance for CFG and use `"a high quality photo"` for conditional generation.

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th>Sample 1</th>
    <th>Sample 2</th>
    <th>Sample 3</th>
    <th>Sample 4</th>
    <th>Sample 5</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/diffusion_cfg_sample_1.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/diffusion_cfg_sample_2.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/diffusion_cfg_sample_3.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/diffusion_cfg_sample_4.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/diffusion_cfg_sample_5.png" alt="Image 4" width="100"></td>
  </tr>
</table>

### Image-to-image Translation

Previously, we take a real image, add noise to it, and then denoise. This effectively allows us to make edits to existing images. The more noise we add, the larger the edit will be. This works because in order to denoise an image, the diffusion model must to some extent "hallucinate" new things -- the model has to be "creative."

Here, we're going to take the original test image, noise it somehow, and force it back onto the image manifold without any conditioning. Effectively, we're going to get an image that is similar to the test image (with a low-enough noise level), or an image that is far from the test image (with a high-enough noise level). This follows the [SDEdit](https://sde-image-editing.github.io/) algorithm.

We show some results here.

#### Unconditioned Img2Img

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th>Original Image</th>
    <th>SDEdit 20</th>
    <th>SDEdit 10</th>
    <th>SDEdit 07</th>
    <th>SDEdit 05</th>
    <th>SDEdit 03</th>
    <th>SDEdit 01</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/img2img_campanile_00.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_20.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_10.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_07.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_05.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_03.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_01.png" alt="Image 4" width="100"></td>
  </tr>
  <tr>
    <td><img src="/images/DDPM/img2img_campanile_night_00.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_night_20.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_night_10.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_night_07.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_night_05.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_night_03.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_campanile_night_01.png" alt="Image 4" width="100"></td>
  </tr>
  <tr>
    <td><img src="/images/DDPM/img2img_golden_gate_bridge_00.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/img2img_golden_gate_bridge_20.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/img2img_golden_gate_bridge_10.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/img2img_golden_gate_bridge_07.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_golden_gate_bridge_05.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_golden_gate_bridge_03.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_golden_gate_bridge_01.png" alt="Image 4" width="100"></td>
  </tr>
</table>

#### Hand-Drawn Img Editing

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th>Original Image</th>
    <th>SDEdit 20</th>
    <th>SDEdit 10</th>
    <th>SDEdit 07</th>
    <th>SDEdit 05</th>
    <th>SDEdit 03</th>
    <th>SDEdit 01</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/img2img_drawn1_00.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn1_20.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn1_10.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn1_07.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn1_05.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn1_03.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn1_01.png" alt="Image 4" width="100"></td>
  </tr>
  <tr>
    <td><img src="/images/DDPM/img2img_drawn2_00.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn2_20.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn2_10.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn2_07.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn2_05.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn2_03.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn2_01.png" alt="Image 4" width="100"></td>
  </tr>
  <tr>
    <td><img src="/images/DDPM/img2img_drawn3_00.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn3_20.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn3_10.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn3_07.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn3_05.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn3_03.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/img2img_drawn3_01.png" alt="Image 4" width="100"></td>
  </tr>
</table>

#### Text-Conditioned Img2Img

We will do the same thing as SDEdit, but guide the projection with a text prompt. This is no longer pure "projection to the natural image manifold" but also adds control using language. This is simply a matter of changing the prompt from `"a high quality photo"` to any other prompts.

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th>Original Image</th>
    <th>SDEdit 20</th>
    <th>SDEdit 10</th>
    <th>SDEdit 07</th>
    <th>SDEdit 05</th>
    <th>SDEdit 03</th>
    <th>SDEdit 01</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/textimg2img_campanile_00.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_campanile_20.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_campanile_10.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_campanile_07.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_campanile_05.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_campanile_03.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_campanile_01.png" alt="Image 4" width="100"></td>
  </tr>
  <tr>
    <td colspan="7">Text Prompt: "a rocket ship"</td>
  </tr>
  <tr>
    <td><img src="/images/DDPM/textimg2img_cat_00.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_cat_20.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_cat_10.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_cat_07.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_cat_05.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_cat_03.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_cat_01.png" alt="Image 4" width="100"></td>
  </tr>
  <tr>
    <td colspan="7">Text Prompt: "a photo of a dog"</td>
  </tr>
  <tr>
    <td><img src="/images/DDPM/textimg2img_ironman_00.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_ironman_20.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_ironman_10.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_ironman_07.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_ironman_05.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_ironman_03.png" alt="Image 4" width="100"></td>
    <td><img src="/images/DDPM/textimg2img_ironman_01.png" alt="Image 4" width="100"></td>
  </tr>
  <tr>
    <td colspan="7">Text Prompt: "a man wearing a hat"</td>
  </tr>
</table>

#### In-Painting

We can use the same procedure to implement inpainting (following the [RePaint](https://arxiv.org/abs/2201.09865) paper). That is, given an image $x_{\text{orig}}$ and a binary mask $\mathbf{m}$, we can create a new image that has the same content where $\mathbf{m} = 0$, but new content wherever $\mathbf{m} = 1$.

To do this, we can run the diffusion denoising loop. But at every step, after obtaining $x_t$, we "force" $x_t$ to have the same pixels as $x_{\text{orig}}$ where $\mathbf{m} = 0$, i.e.:

$$
x_t \leftarrow \mathbf{m} x_t + (1 - \mathbf{m}) \text{forward}(x_{\text{orig}}, t)
$$

Essentially, we leave everything inside the edit mask alone, but we replace everything outside the edit mask with our original image -- with the correct amount of noise added for timestep $t$.

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th>Original Image</th>
    <th>Mask</th>
    <th>Hole to Fill</th>
    <th>Inpainted Image</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/inpaint1_image.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/inpaint1_mask.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/inpaint1_toreplace.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/inpaint1_result.png" alt="Image 4" width="100"></td>
  </tr>
  <tr>
    <td><img src="/images/DDPM/inpaint2_image.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/inpaint2_mask.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/inpaint2_toreplace.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/inpaint2_result.png" alt="Image 4" width="100"></td>
  </tr>
  <tr>
    <td><img src="/images/DDPM/inpaint3_image.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/inpaint3_mask.png" alt="Image 2" width="100"></td>
    <td><img src="/images/DDPM/inpaint3_toreplace.png" alt="Image 3" width="100"></td>
    <td><img src="/images/DDPM/inpaint3_result.png" alt="Image 4" width="100"></td>
  </tr>
</table>

### Visual Anagrams

In this part, we are finally ready to implement [Visual Anagrams](https://dangeng.github.io/visual_anagrams/) and create optical illusions with diffusion models. In this part, we will create an image that looks like _"an oil painting of an old man"_, but when flipped upside down will reveal _"an oil painting of people around a campfire"_.

To do this, we will denoise an image $x_t$ at step $t$ normally with the prompt _"an oil painting of an old man"_, to obtain noise estimate $\epsilon_1$. But at the same time, we will flip $x_t$ upside down, and denoise with the prompt _"an oil painting of people around a campfire"_, to get noise estimate $\epsilon_2$. We can flip $\epsilon_2$ back, to make it right-side up, and average the two noise estimates. We can then perform a reverse/denoising diffusion step with the averaged noise estimate.

The full algorithm will be:

$$
\epsilon_1 = \text{UNet}(x_t, t, p_1) \\
\epsilon_2 = \text{flip}(\text{UNet}(\text{flip}(x_t), t, p_2))\\
\epsilon = \frac{\epsilon_1 + \epsilon_2}{2}
$$

Where UNet is the diffusion model UNet from before, $\text{flip}(\cdot)$ is a function that flips the image, and $p_1$ and $p_2$ are two different text prompt embeddings. And our final noise estimate is $\epsilon$.

Here are some example results produced.

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th>An oil painting of an old man</th>
    <th>An oil painting of people around a campfire</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/flip11.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/flip12.png" alt="Image 2" width="100"></td>
  </tr>
  <tr>
    <th>An oil painting of a snowy mountain village</th>
    <th>A photo of a hipster barista</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/flip21.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/flip22.png" alt="Image 2" width="100"></td>
  </tr>
  <tr>
    <th>A photo of the amalfi coast</th>
    <th>An oil painting of a snowy mountain village</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/flip31.png" alt="Image 1" width="100"></td>
    <td><img src="/images/DDPM/flip32.png" alt="Image 2" width="100"></td>
  </tr>
</table>

### Hybrid Images

In this part, we'll implement [Factorized Diffusion](https://arxiv.org/abs/2404.11615) and create hybrid images.

In order to create hybrid images with a diffusion model, we can use a similar technique as above. We will create a composite noise estimate $\epsilon$, by estimating the noise with two different text prompts, and then combining low frequencies from one noise estimate with high frequencies of the other. The algorithm is:

$$
\epsilon_1 = \text{UNet}(x_t, t, p_1) \\
\epsilon_2 = \text{UNet}(x_t, t, p_2) \\
\epsilon = f_{\text{lowpass}}(\epsilon_1) + f_{\text{highpass}}(\epsilon_2)
$$

Where UNet is the diffusion model UNet, $f_{\text{lowpass}}$ is a low-pass function, $f_{\text{highpass}}$ is a high-pass function, and $p_1$ and $p_2$ are two different text prompt embeddings. Our final noise estimate is $\epsilon$. 

<table border="1" style="border-collapse: collapse; text-align: center;">
  <tr>
    <th>A lithograph of a skull</th>
    <th>A lithograph of waterfalls</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/hybrid11.png" alt="Image 1" width="50"></td>
    <td><img src="/images/DDPM/hybrid12.png" alt="Image 2" width="300"></td>
  </tr>
  <tr>
    <th>A lithograph of a skull</th>
    <th>An oil painting of a snowy mountain village</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/hybrid21.png" alt="Image 1" width="50"></td>
    <td><img src="/images/DDPM/hybrid22.png" alt="Image 2" width="300"></td>
  </tr>
  <tr>
    <th>An oil painting of an old man</th>
    <th>An oil painting of people around a campfire</th>
  </tr>
  <tr>
    <td><img src="/images/DDPM/hybrid31.png" alt="Image 1" width="50"></td>
    <td><img src="/images/DDPM/hybrid32.png" alt="Image 2" width="300"></td>
  </tr>
</table>

## Part B: Training Your Own Diffusion Model!

### Train a Single-Step Denoising UNet

Let's warm up by building a simple one-step denoiser. Given a noisy image $z$, we aim to train a denoiser $D_\theta$ such that it maps $z$ to a clean image $x$. To do so, we can optimize over an L2 loss:

$$
L = \mathbb{E}_{z, x} \|D_\theta(z) - x\|^2
$$

#### Implementing the UNet

In this project, we implement the denoiser as a [UNet](https://arxiv.org/abs/1505.04597). It consists of a few downsampling and upsampling blocks with skip connections.

![](/images/DDPM/unconditional_arch.png)

The diagram above uses a number of standard tensor operations defined as follows:

![](/images/DDPM/atomic_ops_new.png)

####  Forward Process

Recall from equation 1 that we aim to solve the following denoising problem: Given a noisy image $z$, we aim to train a denoiser $D_\theta$ such that it maps $z$ to a clean image $x$. To do so, we can optimize over an L2 loss:

$$
L = \mathbb{E}_{z, x} \|D_\theta(z) - x\|^2.
$$

To train our denoiser, we need to generate training data pairs of $(z, x)$, where each $x$ is a clean MNIST digit. For each training batch, we can generate $z$ from $x$ using the following noising process:

$$
z = x + \sigma \epsilon, \quad \text{where} \quad \epsilon \sim N(0, \mathbf{I}).
$$

Visualize the different noising processes over $\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]$, assuming normalized $x \in [0, 1]$, would get something like the following results.

![](/images/DDPM/MNIST_forward.png)

#### Training Details

Now, we will train the model to perform denoising.

- **Objective**: Train a denoiser to denoise noisy image $z$ with $\sigma = 0.5$ applied to a clean image $x$.
- **Dataset and dataloader**: Use the MNIST dataset via `torchvision.datasets.MNIST` with flags to access training and test sets. Train only on the training set. Shuffle the dataset before creating the dataloader. Recommended batch size: 256. We'll train over our dataset for 5 epochs.
  - You should only noise the image batches when fetched from the dataloader so that in every epoch the network will see new noised images, improving generalization.
- **Model**: Use the UNet architecture defined in section 1.1 with recommended hidden dimension $D = 128$.
- **Optimizer**: Use Adam optimizer with learning rate of $10^{-4}$.

We can get some loss curve like the following graph:

![](/images/DDPM/loss_curve_1.png)

Also, we can visualize after some epoch of training.

<table border="1" style="border-collapse: collapse; text-align: center; width: 100%;">
  <tr>
    <td>Original<br>Images</td>
    <td><img src="/images/DDPM/mnist_orig.png" alt="Image 1" width="500"></td>
  </tr>
  <tr>
    <td>Noised<br>Images</td>
    <td><img src="/images/DDPM/mnist_noised.png" alt="Image 2" width="500"></td>
  </tr>
  <tr>
    <td>Denoised<br>Train epoch=1</td>
    <td><img src="/images/DDPM/mnist_denoised_1epoch.png" alt="Image 3" width="500"></td>
  </tr>
  <tr>
    <td>Denoised<br>Train epoch=5</td>
    <td><img src="/images/DDPM/mnist_denoised_5epoch.png" alt="Image 4" width="500"></td>
  </tr>
</table>

#### Out-of-Distribution Testing

Our denoiser was trained on MNIST digits noised with $\sigma=0.5$. Let's see how the denoiser performs on different $\sigma$'s that it wasn't trained for.

Visualize the denoiser results on test set digits with varying levels of noise $\sigma=[0.0,0.2,0.4,0.5,0.6,0.8,1.0]$.

<img src="/images/DDPM/mnist_denoised_sigma0.png" alt="Image 1" width="60%">

<img src="/images/DDPM/mnist_denoised_sigma0.2.png" alt="Image 1" width="60%">

<img src="/images/DDPM/mnist_denoised_sigma0.4.png" alt="Image 1" width="60%">

<img src="/images/DDPM/mnist_denoised_sigma0.6.png" alt="Image 1" width="60%">

<img src="/images/DDPM/mnist_denoised_sigma0.8.png" alt="Image 1" width="60%">

<img src="/images/DDPM/mnist_denoised_sigma1.0.png" alt="Image 1" width="60%">

### Train a Diffusion Model

Now, we are ready for diffusion, where we will train a UNet model that can iteratively denoise an image. We will implement [DDPM](https://arxiv.org/abs/2006.11239) in this part.

Let's revisit the problem we solved previously:

$$
L = \mathbb{E}_{z, x} \|D_\theta(z) - x\|^2.
$$

We will first introduce one small difference: we can change our UNet to predict the added noise $\epsilon$ instead of the clean image $x$. Mathematically, these are equivalent since $x = z - \sigma \epsilon$. Therefore, we can turn equation into the following:

$$
L = \mathbb{E}_{\epsilon, z} \|\epsilon_\theta(z) - \epsilon\|^2
$$

Where $\epsilon_\theta$ is a UNet trained to predict noise.

For diffusion, we eventually want to sample a pure noise image $\epsilon \sim N(0, I)$ and generate a realistic image $x$ from the noise. However, we saw in part A that one-step denoising does not yield good results. Instead, we need to **iteratively** denoise the image for better results.

Recall in part A that we used equation A.2 to generate noisy images $x_t$ from $x_0$ for some timestep $t$ for $t \in \{0, 1, \cdots, T\}$:

$$
x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon
\quad \text{where} \quad \epsilon \sim N(0, 1).
$$

Intuitively, when $t = 0$ we want $x_t$ to be the clean image $x_0$, and for $t = T$ we want $x_t$ to be pure noise $\epsilon$, and for $t \in \{1, \cdots, T - 1\}$, $x_t$ should be some linear combination of the two. The precise derivation of $\bar{\alpha}_t$ is beyond the scope of this project (see [DDPM paper](https://arxiv.org/abs/2006.11239) for more details). Here, we provide you with the DDPM recipe to build a list $\bar{\alpha}_t$ for $t \in \{0, 1, \cdots, T\}$ utilizing lists $\alpha$ and $\beta$:

- Create a list $\beta$ of length $T$ such that $\beta_0 = 0.0001$ and $\beta_T = 0.02$ and all other elements $\beta_t$ for $t \in \{1, \cdots, T - 1\}$ are evenly spaced between the two.
- $\alpha_t = 1 - \beta_t$
- $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$ is a cumulative product of $\alpha_s$ for $s \in \{1, \cdots, t\}$.

Because we are working with simple MNIST digits, we can afford to have a smaller $T$ of 300 instead of the 1000 used in part A. Observe how $\bar{\alpha}_t$ is close to 1 for small $t$ and close to 0 for larger $T$. $\beta$ is known as the variance schedule; it controls the amount of noise added at each timestep.

Now, to denoise image $x_t$, we could simply apply our UNet $\epsilon_\theta(x_t)$ on $x_t$ and get the noise $\epsilon$. However, this won't work very well because the UNet is expecting the noisy image to have a noise variance $\sigma = 0.5$ for best results, but the variance of $x_t$ varies with $t$. One could train $T$ separate UNets, but it is much easier to simply condition a single UNet with timestep $t$, giving us our final objective:

$$
L = \mathbb{E}_{\epsilon, x_0, t} \|\epsilon_\theta(x_t, t) - \epsilon\|^2
$$

#### Adding Time Conditioning to UNet

We need a way to inject scalar $t$ into our UNet model to condition it.

![](/images/DDPM/conditional_arch_1.png)

This uses a new operator called `FCBlock` (fully-connected block) which we use to inject the conditioning signal into the UNet:

<img src="/images/DDPM/fc_long.png" alt="image" style="zoom: 33%;" />

#### Train the Time Conditioned UNet

Training our time-conditioned UNet $\epsilon_\theta(x_t, t)$ is now pretty easy. Basically, we pick a random image from the training set, a random $t$, and train the denoiser to predict the noise in $x_t$. We repeat this for different images and different t values until the model converges and we are happy.

<img src="/images/DDPM/algo1_t_only.png" alt="image" style="zoom: 33%;" />

- **Objective**: Train a time-conditioned UNet $\epsilon_\theta(x_t, t)$ to predict the noise in $x_t$ given a noisy image $x_t$ and a timestep $t$.
  
- **Dataset and dataloader**: Use the MNIST dataset via `torchvision.datasets.MNIST` with flags to access training and test sets. Train only on the training set. Shuffle the dataset before creating the dataloader. Recommended batch size: 128. We'll train over our dataset for 20 epochs since this task is more difficult than part A.
  
  - As shown in algorithm B.1, you should only noise the image batches when fetched from the dataloader.
  
- **Model**: Use the time-conditioned UNet architecture defined in section 2.1 with recommended hidden dimension $D = 64$. Follow the diagram and pseudocode for how to inject the conditioning signal $t$ into the UNet. Remember to normalize $t$ before embedding it.

- **Optimizer**: Use Adam optimizer with an initial learning rate of $1 \times 10^{-3}$. We will be using an exponential learning rate scheduler with a gamma of $0.1^{(1.0 / \text{num\_epochs})}$. This can be implemented using:

  ```python
  scheduler = torch.optim.lr_scheduler.ExponentialLR(...)

Here is the Time-Conditioned UNet training loss curve.

![](/images/DDPM/loss_curve_2.png)

#### Sampling from the Time Conditioned UNet

The sampling process is very similar to part A, except we don't need to predict the variance like in the DeepFloyd model. Instead, we can use our list $\beta$.

<table border="1" style="border-collapse: collapse; text-align: center; width: 100%;">
  <tr>
    <td>Sampled Image<br>Train epoch=5</td>
    <td><img src="/images/DDPM/TC_Unet_sample_5.png" alt="Image 3" width="500"></td>
  </tr>
  <tr>
    <td>Sampled Image<br>Train epoch=20</td>
    <td><img src="/images/DDPM/TC_Unet_sample_20.png" alt="Image 4" width="500"></td>
  </tr>
</table>

#### Adding Class-Conditioning to UNet

To make the results better and give us more control for image generation, we can also optionally condition our UNet on the class of the digit 0-9. This will require adding 2 more `FCBlock`s to our UNet, but we suggest that for class-conditioning vector $c$, you make it a one-hot vector instead of a single scalar. 

Because we still want our UNet to work without it being conditioned on the class, we implement dropout where 10% of the time $(p_{\text{uncond}} = 0.1)$, we drop the class conditioning vector $c$ by setting it to 0. Here is one way to condition our UNet $\epsilon_\theta(x_t, t, c)$ on both time $t$ and class $c$: `unflatten = c1 * unflatten + t1`

Training for this section will be the same as time-only, with the only difference being the conditioning vector c and doing unconditional generation periodically.

<img src="/images/DDPM/algo3_c.png" alt="images" style="zoom:50%;" />

The training loss can be visualized as:

![](/images/DDPM/loss_curve_3.png)

<table border="1" style="border-collapse: collapse; text-align: center; width: 100%;">
  <tr>
    <td>Sampled Image<br>Train epoch=5</td>
    <td><img src="/images/DDPM/image-20241119225338968.png" alt="Image 3" width="500"></td>
  </tr>
  <tr>
    <td>Sampled Image<br>Train epoch=20</td>
    <td><img src="/images/DDPM/image-20241119225347464.png" alt="Image 4" width="500"></td>
  </tr>
</table>

It seems great! We can say that this little diffusion model has "learned" to writing numbers!

